{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.2: Experiment Tracking for LLMs\n",
    "\n",
    "![](images/3_Notebook-12-LLM-Experiment-Tracking.png)\n",
    "\n",
    "\n",
    "## Tracking GenAI Experiments with MLflow\n",
    "\n",
    "Welcome to the second notebook! Now that your environment is set up, you'll learn how to track LLM experiments systematically.\n",
    "\n",
    "### What You'll Learn\n",
    "- How `mlflow.openai.autolog()` captures model params, tokens, latency, and I/O automatically\n",
    "- When you still need explicit `mlflow.log_*` calls (tags, custom artifacts)\n",
    "- How to create and organize GenAI experiments\n",
    "- Compare different LLM configurations\n",
    "- Best practices for experiment organization\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebook 1.1 (Setup)\n",
    "- MLflow >= 3.10.0\n",
    "- MLflow UI running (recommended)\n",
    "\n",
    "### Estimated Time: 25-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Environment Setup\n",
    "\n",
    "Let's load our environment and enable autologging for OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "from utils.clnt_utils import get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names, is_databricks_ai_gateway_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_ai_gateway = is_databricks_ai_gateway_client()\n",
    "\n",
    "# Verify which client to use\n",
    "if use_ai_gateway:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "else:\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5.2\"\n",
    "\n",
    "# Verify OpenAI key\n",
    "if not use_ai_gateway and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "# Enable autologging for OpenAI.\n",
    "# This automatically creates MLflow Traces that capture:\n",
    "#   - model, temperature, max_tokens (all API params as span attributes)\n",
    "#   - full input messages and response content (span I/O)\n",
    "#   - token counts: prompt_tokens, completion_tokens, total_tokens\n",
    "#   - latency (via span start/end timestamps)\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"‚úÖ Environment configured successfully\")\n",
    "print(f\"   MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Using model: {model_name}\")\n",
    "print(\"   Autolog: ENABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding Experiment Tracking\n",
    "\n",
    "### What is Experiment Tracking?\n",
    "\n",
    "Experiment tracking captures the inputs, outputs, and context of your LLM experiments:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              EXPERIMENT                          ‚îÇ\n",
    "‚îÇ  Name: \"sentiment-analysis\"                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                  ‚îÇ\n",
    "‚îÇ  RUN 1: gpt-5.2, temp=1.0                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Parameters: {model, temperature, ...}        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Metrics: {accuracy, latency, cost}           ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Artifacts: {prompt.txt, config.json}         ‚îÇ\n",
    "‚îÇ                                                  ‚îÇ\n",
    "‚îÇ  RUN 2: gpt-5.2, temp=1.5                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Parameters: {model, temperature, ...}        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Metrics: {accuracy, latency, cost}           ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Artifacts: {prompt.txt, config.json}         ‚îÇ\n",
    "‚îÇ                                                  ‚îÇ\n",
    "‚îÇ  RUN 3: gpt-5.2, temp=2.0                        ‚îÇ\n",
    "‚îÇ  ...                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Parameters**: Configuration values (model name, temperature, max_tokens)\n",
    "- **Metrics**: Numerical measurements (accuracy, latency, token count)\n",
    "- **Artifacts**: Files (prompts, responses, model configs)\n",
    "- **Tags**: Metadata for organizing and filtering runs\n",
    "- **Traces**: Automatic records of LLM calls (created by autolog)\n",
    "\n",
    "### What Does `mlflow.openai.autolog()` Capture?\n",
    "\n",
    "Since we enabled autolog in Step 1, every OpenAI call is automatically traced. Here is what you get for free vs. what still needs manual logging:\n",
    "\n",
    "| Captured Automatically (Traces)             | Requires Explicit `log_*` Calls          |\n",
    "|---------------------------------------------|------------------------------------------|\n",
    "| model, temperature, max_tokens (span attrs) | Estimated cost in USD                    |\n",
    "| Full input messages (span I/O)              | Semantic tags (task, stage, team, etc.)  |\n",
    "| Full response content (span I/O)            | Structured config artifacts (`log_dict`) |\n",
    "| Token counts: prompt, completion, total     | Custom business metrics                  |\n",
    "| Latency (span start/end timestamps)         |                                          |\n",
    "\n",
    "**The rest of this notebook demonstrates each category ‚Äî you'll see when a `log_*` call earns its keep.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Your First Tracked LLM Call ‚Äî The Autolog Way\n",
    "\n",
    "Let's see what autolog captures with zero manual instrumentation, then add only what it cannot provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "experiment_name = \"02-basic-llm-calls\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"üìä Experiment: {experiment_name}\")\n",
    "print(\"   View in UI: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tracked LLM call ‚Äî autolog does the heavy lifting.\n",
    "#\n",
    "# What we do NOT need to log manually (autolog captures all of this):\n",
    "#   - mlflow.log_param(\"model\", ...)        -> span attribute\n",
    "#   - mlflow.log_param(\"temperature\", ...)   -> span attribute\n",
    "#   - mlflow.log_param(\"max_tokens\", ...)    -> span attribute\n",
    "#   - mlflow.log_metric(\"latency_seconds\")   -> span timestamps\n",
    "#   - mlflow.log_metric(\"prompt_tokens\")     -> mlflow.chat.tokenUsage\n",
    "#   - mlflow.log_metric(\"completion_tokens\") -> mlflow.chat.tokenUsage\n",
    "#   - mlflow.log_metric(\"total_tokens\")      -> mlflow.chat.tokenUsage\n",
    "#   - mlflow.log_text(prompt, \"prompt.txt\")  -> span input\n",
    "#   - mlflow.log_text(answer, \"response.txt\")-> span output\n",
    "\n",
    "prompt = \"Explain MLflow GenAI Platform in 3-4 sentences.\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"first-llm-tracked-call\") as run:\n",
    "\n",
    "    # The only explicit call: a semantic tag that autolog cannot infer.\n",
    "    mlflow.set_tag(\"task\", \"explanation\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "        max_completion_tokens=1000\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "print(f\"\\nüìù Prompt: {prompt}\")\n",
    "print(f\"\\nü§ñ Response: {answer}\")\n",
    "print(f\"\\nüîó Run ID: {run.info.run_id}\")\n",
    "print(f\"   View in UI: http://localhost:5000/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ What Just Happened?\n",
    "\n",
    "1. **One tag** Use `set_tag(\"task\", ...)` because that is a semantic label *we* know ‚Äî autolog has no way to infer it.\n",
    "\n",
    "2. **Autolog created a Trace automatically.** In the MLflow UI, go to the **Traces** tab of the `02-basic-llm-calls` experiment. You will see:\n",
    "   - `model`, `temperature`, `max_tokens` as span attributes\n",
    "   - The full prompt and response captured as span I/O\n",
    "   - Token counts under `mlflow.chat.tokenUsage`\n",
    "   - Token costs for input and ouput\n",
    "   - Latency derived from span start/end timestamps\n",
    "\n",
    "3. **The Trace is linked to the Run** via `mlflow.sourceRun`. You can navigate from the Trace back to the Run and vice versa.\n",
    "\n",
    "**Try it:** Open the MLflow UI. Find the run. Click into its Trace. Compare what autolog captured vs. what we logged manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Comparing Multiple Configurations\n",
    "\n",
    "With autolog active, our comparison helper needs only to make the API call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified helper ‚Äî autolog captures params, tokens, latency, and I/O automatically.\n",
    "def simple_llm_call(prompt, model=model_name, temperature=1.0, max_completion_tokens=1000, run_name=None):\n",
    "    \"\"\"\n",
    "    Make an LLM call inside a nested run.\n",
    "    autolog captures model params, token counts, latency, and full I/O as a Trace.\n",
    "    The run exists only to group and name the experiment.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name, nested=True):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_completion_tokens=max_completion_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "print(\"‚úÖ Helper function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment for comparison\n",
    "mlflow.set_experiment(\"02-temperature-comparison\")\n",
    "\n",
    "test_prompt = \"Write a creative tagline for an AI observability with MLflow GenAI platform.\"\n",
    "temperatures = [1.0, 1.5, 2.0]\n",
    "\n",
    "print(\"üî¨ Running temperature comparison...\\n\")\n",
    "\n",
    "# A parent run groups all nested calls together in the UI.\n",
    "with mlflow.start_run(run_name=\"temperature-sweep\"):\n",
    "    mlflow.set_tag(\"sweep_variable\", \"temperature\")\n",
    "\n",
    "    for temp in temperatures:\n",
    "        print(f\"  temperature={temp} ...\")\n",
    "        response = simple_llm_call(\n",
    "            prompt=test_prompt,\n",
    "            model=model_name,\n",
    "            temperature=temp,\n",
    "            max_completion_tokens=1000,\n",
    "            run_name=f\"temp_{temp}\"\n",
    "        )\n",
    "        print(f\"    -> {response}\\n\")\n",
    "\n",
    "print(\"‚úÖ Done. Compare traces side-by-side in the MLflow UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Analysis\n",
    "\n",
    "Notice how temperature affects:\n",
    "- **Creativity**: Higher temperature = more creative responses\n",
    "- **Consistency**: Lower temperature = more deterministic\n",
    "- **Token usage**: May vary with creativity level\n",
    "\n",
    "**üí° Comparing in the MLflow UI:**\n",
    "1. Select the \"02-temperature-comparison\" experiment\n",
    "2. Select the nested runs and click \"Compare\"\n",
    "3. Because autolog captured `temperature` as a span attribute on every Trace, you can also filter by temperature directly in the **Traces** tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Tracking Cost Estimates\n",
    "\n",
    "MLflow 3.10 traces now account for how much a trace costs, broken down into input costs and output cost.\n",
    "This helps you to ascertain the overall costs while trying out different models, either from the same provider\n",
    "or a different provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_with_cost(prompt, model=model_name, temperature=1.0, max_completion_tokens=1000, run_name=None):\n",
    "    \"\"\"\n",
    "    Make an LLM call with cost tracking.\n",
    "    autolog captures: model, temperature, token counts, latency, I/O.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_completion_tokens=max_completion_tokens\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "\n",
    "        return answer\n",
    "\n",
    "print(\"‚úÖ Cost-aware helper defined\")\n",
    "print(\"   autolog captures: model, temperature, token counts, latency, I/O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare costs across different models\n",
    "mlflow.set_experiment(\"03-model-cost-comparison\")\n",
    "\n",
    "prompt = \"Summarize the benefits of experiment tracking in 3 bullet points.\"\n",
    "models_to_test = [\"jsd-gpt-5-2\", \"jsd-gpt-5-mini\"] if use_ai_gateway else [\"gpt-5-mini\", \"gpt-5.2\"]\n",
    "\n",
    "print(\"üí∞ Comparing costs across models...\\n\")\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"Testing {model}...\")\n",
    "    response = llm_call_with_cost(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        temperature=1.0,\n",
    "        max_completion_tokens=1000,\n",
    "        run_name=f\"model_{model}_run\"\n",
    "    )\n",
    "    print(f\"  Response: {response}...\\n\")\n",
    "\n",
    "print(\"‚úÖ Cost comparison complete! View in MLflow UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Cost Analysis Insights\n",
    "\n",
    "By tracking costs, you can:\n",
    "1. **Budget effectively** for production deployments\n",
    "2. **Optimize model selection** (GPT-5-mini vs GPT-5.2)\n",
    "3. **Identify expensive prompts** that need optimization\n",
    "4. **Track spending trends** over time\n",
    "\n",
    "**Note:** Token counts themselves come from autolog Traces (`mlflow.chat.tokenUsage`). The only value `estimated_cost_usd` adds is the dollar figure, which requires pricing data only you can supply. This is the correct pattern: **log exactly what observability infrastructure cannot derive on its own.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Organizing Experiments with Tags and Metadata\n",
    "\n",
    "Tags and structured configs are another area where explicit logging adds real value ‚Äî autolog has no way to know your team structure, production candidacy, or versioning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systematic experiment with rich metadata ‚Äî tags and config artifacts.\n",
    "mlflow.set_experiment(\"04-production-candidate-testing\")\n",
    "\n",
    "# Test configurations\n",
    "open_configs = [\n",
    "    {\n",
    "        \"name\": \"baseline\",\n",
    "        \"model\": \"gpt-5-mini\",\n",
    "        \"temperature\": 1.0,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"creative\",\n",
    "        \"model\": \"gpt-5.2\",\n",
    "        \"temperature\": 2.0,\n",
    "        \"system_prompt\": \"You are a creative writing assistant.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Databricks hosted foundational models if you want to test them\n",
    "databricks_config = [\n",
    "    {\n",
    "        \"name\": \"baseline\",\n",
    "        \"model\": \"jsd-gpt-5-mini\",\n",
    "        \"temperature\": 1.0,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"creative\",\n",
    "        \"model\": \"jsd-gpt-5.2\",\n",
    "        \"temperature\": 1.5,\n",
    "        \"system_prompt\": \"You are a creative writing assistant.\"\n",
    "    },\n",
    "]\n",
    "model_configs = databricks_config if use_ai_gateway else open_configs\n",
    "test_prompt = \"Explain the concept of LLM temperature.\"\n",
    "\n",
    "print(\"üè∑Ô∏è  Running experiments with semantic tags...\\n\")\n",
    "\n",
    "for config in model_configs:\n",
    "    with mlflow.start_run(run_name=config[\"name\"]):\n",
    "\n",
    "        # Make the call ‚Äî autolog captures model, temperature, tokens, I/O, latency.\n",
    "        response = client.chat.completions.create(\n",
    "            model=config[\"model\"],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": config[\"system_prompt\"]},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            temperature=config[\"temperature\"],\n",
    "            max_completion_tokens=1000\n",
    "        )\n",
    "\n",
    "        # Log only what autolog cannot provide: semantic tags and structured config.\n",
    "        mlflow.set_tags({\n",
    "            \"config_name\": config[\"name\"],\n",
    "            \"task\": \"explanation\",\n",
    "            \"stage\": \"testing\",\n",
    "            \"team\": \"ai-research\",\n",
    "            \"version\": \"v1.0\",\n",
    "            \"production_candidate\": str(config[\"name\"] == \"baseline\").lower(),\n",
    "        })\n",
    "\n",
    "        # Save full config as a structured artifact\n",
    "        mlflow.log_dict(config, \"config.json\")\n",
    "\n",
    "        print(f\"  ‚úì {config['name']} done\")\n",
    "\n",
    "print(\"\\n‚úÖ All runs completed! Filter by tag 'production_candidate=true' in the UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè∑Ô∏è Tagging Best Practices\n",
    "\n",
    "Use tags for:\n",
    "1. **Environment**: `stage: development/testing/production`\n",
    "2. **Ownership**: `team: ai-research`, `owner: jules`\n",
    "3. **Purpose**: `task: summarization`, `use_case: customer-support`\n",
    "4. **Status**: `production_candidate: true`, `approved: false`\n",
    "5. **Version**: `version: v1.0`, `prompt_version: v2.1`\n",
    "6. **Do not duplicate autolog data.** Tags like `model_used: gpt-5.2` or `total_tokens: 342` are already in the Trace. Reserve tags for information that is not derivable from the API call itself.\n",
    "\n",
    "**üí° You can filter and search runs by tags in the MLflow UI!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Querying Experiments Programmatically\n",
    "\n",
    "Let's learn how to retrieve and analyze experiment data using the MLflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Use the MlflowClient to query experiments and runs\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "# Get experiment by name\n",
    "experiment = mlflow_client.get_experiment_by_name(\"04-production-candidate-testing\")\n",
    "\n",
    "if experiment:\n",
    "    print(f\"üìä Experiment: {experiment.name}\")\n",
    "    print(f\"   ID: {experiment.experiment_id}\")\n",
    "\n",
    "    # Search runs ‚Äî sort by start_time since autolog stores latency on Traces, not run metrics.\n",
    "    runs = mlflow_client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=5\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   Found {len(runs)} runs:\\n\" + \"=\"*60)\n",
    "\n",
    "    for run in runs:\n",
    "        print(f\"\\n   Run: {run.info.run_name}\")\n",
    "        if run.data.params:\n",
    "            print(\"   Parameters:\")\n",
    "            for key, value in run.data.params.items():\n",
    "                print(f\"      {key}: {value}\")\n",
    "        if run.data.metrics:\n",
    "            print(\"   Metrics:\")\n",
    "            for key, value in run.data.metrics.items():\n",
    "                print(f\"      {key}: {value}\")\n",
    "        if run.data.tags.get(\"config_name\"):\n",
    "            print(f\"   Tag config_name: {run.data.tags['config_name']}\")\n",
    "else:\n",
    "    print(\"Experiment not found. Make sure you ran the production candidate testing section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find production candidates using tag filters\n",
    "if experiment:\n",
    "    prod_runs = mlflow_client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=\"tags.production_candidate = 'true'\",\n",
    "        max_results=5\n",
    "    )\n",
    "\n",
    "    print(\"üèÜ Production Candidates:\")\n",
    "    for run in prod_runs:\n",
    "        print(f\"   Name: {run.info.run_name}\")\n",
    "        print(f\"   Config: {run.data.tags.get('config_name', 'N/A')}\")\n",
    "        print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Home work: Advanced Queries\n",
    "\n",
    "**Querying Runs** (explicit `log_*` data lives here):\n",
    "\n",
    "```python\n",
    "# Filter by metric threshold\n",
    "cheap_runs = mlflow_client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"metrics.estimated_cost_usd < 0.001\"\n",
    ")\n",
    "\n",
    "# Filter by tag\n",
    "prod_candidates = mlflow_client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"tags.production_candidate = 'true'\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Querying Traces** (autolog data lives here):\n",
    "\n",
    "```python\n",
    "# Search traces for an experiment\n",
    "traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    ")\n",
    "\n",
    "# Get all traces linked to a specific run\n",
    "traces = mlflow.search_traces(\n",
    "    run_id=run_id\n",
    ")\n",
    "```\n",
    "\n",
    "Since autolog stores model params, token counts, and latency on Traces rather than run metrics, use `mlflow.search_traces()` when you need to query that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ‚úÖ How `mlflow.openai.autolog()` captures model params, tokens, latency, and I/O as Traces\n",
    "2. ‚úÖ When to use explicit `mlflow.log_*` calls (cost, tags, config artifacts, semantic outputs)\n",
    "3. ‚úÖ Comparing multiple model configurations with minimal boilerplate\n",
    "4. ‚úÖ Tracking costs ‚Äî Now tracked automatically in MLflow 3.10\n",
    "5. ‚úÖ Organizing experiments with tags and metadata\n",
    "6. ‚úÖ Querying runs and traces programmatically\n",
    "\n",
    "### What to Log and What to Skip\n",
    "\n",
    "| Category                        | Autolog? | Explicit `log_*`? |\n",
    "|---------------------------------|:--------:|:------------------:|\n",
    "| Model, temperature, max_tokens  | ‚úÖ auto  | No ‚Äî redundant     |\n",
    "| Token counts (prompt/completion) | ‚úÖ auto | No ‚Äî redundant     |\n",
    "| Latency                         | ‚úÖ auto (span timestamps) | No ‚Äî redundant |\n",
    "| Full prompt & response          | ‚úÖ auto (span I/O) | No ‚Äî redundant |\n",
    "| Semantic tags (task, stage, team)| ‚ùå      | ‚úÖ YES             |\n",
    "| Structured config artifacts     | ‚ùå       | ‚úÖ YES             |\n",
    "| Cross-step summaries            | ‚ùå       | ‚úÖ YES             |\n",
    "| Semantic output params          | ‚ùå       | ‚úÖ YES             |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Ready to dive deep into observability?\n",
    "\n",
    "**üìì Notebook 1.3: Introduction to Tracing**\n",
    "- Learn automatic tracing with autologging\n",
    "- Understand the trace data model\n",
    "- Visualize LLM execution flows\n",
    "- Integrate with multiple frameworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
