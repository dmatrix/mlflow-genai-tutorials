{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.3: Introduction to Tracing\n",
    "\n",
    "![](images/4_Notebook-13-Introduction-to-Tracing.png)\n",
    "\n",
    "## LLM Observability for GenAI Applications\n",
    "\n",
    "Welcome to the tracing section! This is where MLflow really shines for GenAI development. You'll learn how to gain complete visibility into your LLM applications.\n",
    "\n",
    "### What You'll Learn\n",
    "- What is tracing and why it matters for GenAI\n",
    "- Understanding the trace data model (traces, spans, hierarchy)\n",
    "- Automatic tracing with autologging\n",
    "- Tracing multiple frameworks (OpenAI, LangChain)\n",
    "- Viewing and analyzing traces in MLflow UI\n",
    "- Debugging with traces uing MLflow Assistant\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebooks 1.1 and 1.2\n",
    "- MLflow UI running\n",
    "- OpenAI API key configured\n",
    "- (Or keys for your foundational model provider, e.g., Databricks)\n",
    "\n",
    "### Estimated Time: 20-25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: What is Tracing?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional experimenting tracking or logging isn't enough for LLM applications:\n",
    "\n",
    "```python\n",
    "# Traditional logging - hard to debug\n",
    "print(\"Calling LLM...\")\n",
    "response = llm.generate(prompt)\n",
    "print(f\"Response: {response}\")\n",
    "# What happened inside? How long did it take? What was sent or received?\n",
    "```\n",
    "\n",
    "### The Solution: Distributed Tracing\n",
    "\n",
    "Tracing captures the **complete execution flow** of your application:\n",
    "\n",
    "```\n",
    "Trace: RAG Application\n",
    "‚îú‚îÄ‚îÄ Span 1: Embed Query [0ms - 200ms]\n",
    "‚îÇ   Input: \"What is MLflow for GenAI?\"\n",
    "‚îÇ   Output: [0.123, 0.456, ...]\n",
    "‚îÇ   \n",
    "‚îú‚îÄ‚îÄ Span 2: Retrieve Documents [200ms - 350ms]\n",
    "‚îÇ   Input: [0.123, 0.456, ...]\n",
    "‚îÇ   Output: [\"MLflow is...\", \"The platform...\"]\n",
    "‚îÇ   \n",
    "‚îî‚îÄ‚îÄ Span 3: Generate Response [350ms - 1500ms]\n",
    "    Input: {query, documents}\n",
    "    Output: \"MLflow is an open source ML and GenAI platform...\"\n",
    "    LLM: gpt-5-mini\n",
    "    Tokens: 150\n",
    "```\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Visibility**: See every step in your LLM workflow\n",
    "2. **Performance**: Identify bottlenecks and latency issues\n",
    "3. **Debugging**: Trace errors to their exact source\n",
    "4. **Cost**: Track token usage per operation\n",
    "5. **Quality**: Inspect inputs/outputs at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding the Trace Data Model\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Trace**: A complete execution of an operation\n",
    "- Represents one request or workflow\n",
    "- Contains one or more spans\n",
    "- Has a root span\n",
    "\n",
    "**Span**: A single operation within a trace\n",
    "- Has a start and end time\n",
    "- Contains inputs and outputs\n",
    "- Has metadata (model, tokens,latecy, etc.)\n",
    "- Can have parent-child relationships\n",
    "\n",
    "**Span Attributes**: Additional metadata\n",
    "- Model name\n",
    "- Token counts\n",
    "- Temperature\n",
    "- Custom attributes\n",
    "\n",
    "### Span Types\n",
    "\n",
    "MLflow defines standard span types:\n",
    "\n",
    "- `CHAIN`: A sequence of operations\n",
    "- `LLM`: Language model call\n",
    "- `RETRIEVER`: Document retrieval\n",
    "- `EMBEDDING`: Text embedding\n",
    "- `TOOL`: Tool/function execution\n",
    "- `AGENT`: Agent reasoning\n",
    "- `PARSER`: Output parsing or generic intermediate parsing of an outcome \n",
    "\n",
    "### Hierarchy Example\n",
    "\n",
    "```\n",
    "TRACE (root)\n",
    "‚îÇ\n",
    "‚îî‚îÄ SPAN: Agent Executor (AGENT)\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ SPAN: Planning Step (LLM)\n",
    "   ‚îÇ  ‚îî‚îÄ attributes: {model: gpt-5, tokens: 50}\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ SPAN: Tool Execution (TOOL)\n",
    "   ‚îÇ  ‚îî‚îÄ attributes: {tool: search, query: \"...\"}\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ SPAN: Final Response (LLM)\n",
    "      ‚îî‚îÄ attributes: {model: gpt-5, tokens: 150}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured: using OpenAI client\n",
      "   MLflow version: 3.10.0\n",
      "   Tracking URI: http://localhost:5000\n",
      "   Using model: gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "else:\n",
    "    # Initialize OpenAI\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-mini\"\n",
    "\n",
    "print(\"‚úÖ Environment configured: using\", \"Databricks\" if use_databricks_provider else \"OpenAI\", \"client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Using model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Your First Trace - Manual Example\n",
    "\n",
    "Before we use autologging, let's understand what tracing captures by without autologging the information. This way we understand the difference and benefits of using autlogging. To some extent, we saw the benefits of autologgin in the previous notebook on experimental tracking LLM calls. So this is just a quick reminder in case you missed that tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/24 10:19:22 INFO mlflow.tracking.fluent: Experiment with name '06-tracing-introduction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Making LLM call WITHOUT tracing...\n",
      "\n",
      "Response: Distributed tracing is a method of instrumenting and recording a request‚Äôs execution path across multiple services by propagating trace context and collecting timestamped spans and metadata to visualize and diagnose latency, errors, and performance bottlenecks.\n",
      "\n",
      "‚ùå What we DON'T know:\n",
      "   - Exact timing of the call\n",
      "   - Detailed request/response structure\n",
      "   - Easy way to correlate with other operations\n",
      "   - Visual representation of execution\n",
      "\n",
      "üîç View trace in MLflow UI: http://localhost:5000\n",
      "   Navigate to: Traces tab\n"
     ]
    }
   ],
   "source": [
    "# Create experiment for tracing examples\n",
    "mlflow.set_experiment(\"06-tracing-introduction\")\n",
    "\n",
    "# Without tracing - basic call\n",
    "print(\"\\nüìù Making LLM call WITHOUT tracing...\\n\")\n",
    "\n",
    "prompt = \"Explain what distributed tracing is in one sentence.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\n‚ùå What we DON'T know:\")\n",
    "print(\"   - Exact timing of the call\")\n",
    "print(\"   - Detailed request/response structure\")\n",
    "print(\"   - Easy way to correlate with other operations\")\n",
    "print(\"   - Visual representation of execution\")\n",
    "\n",
    "# check the trace in the UI\n",
    "print(\"\\nüîç View trace in MLflow UI: http://localhost:5000\")\n",
    "print(\"   Navigate to: Traces tab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Automatic Tracing with OpenAI Autologging\n",
    "\n",
    "Now let's enable tracing with a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI autologging enabled\n",
      "   All OpenAI API calls will now be automatically traced!\n"
     ]
    }
   ],
   "source": [
    "# Enable OpenAI autologging - THIS IS THE MAGIC LINE!\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"‚úÖ OpenAI autologging enabled\")\n",
    "print(\"   All OpenAI API calls will now be automatically traced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Making LLM call WITH tracing...\n",
      "\n",
      "Response: Distributed tracing is a method for tracking and visualizing the end-to-end path and timing of a single request as it travels across multiple services or components to diagnose latency, errors, and dependencies.\n",
      "\n",
      "‚úÖ What we NOW know:\n",
      "   ‚úì Complete request details (model, messages, parameters)\n",
      "   ‚úì Response content and metadata\n",
      "   ‚úì Token usage (prompt, completion, total)\n",
      "   ‚úì Timing information (latency)\n",
      "   ‚úì All captured automatically!\n",
      "\n",
      "üîó View trace in MLflow UI: http://localhost:5000\n",
      "   Navigate to: Traces tab\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-d839fd86a064e3df386509b9a8c9fe88&amp;experiment_id=2&amp;version=3.10.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-d839fd86a064e3df386509b9a8c9fe88)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the same call - now it's automatically traced!\n",
    "print(\"\\nüîç Making LLM call WITH tracing...\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\n‚úÖ What we NOW know:\")\n",
    "print(\"   ‚úì Complete request details (model, messages, parameters)\")\n",
    "print(\"   ‚úì Response content and metadata\")\n",
    "print(\"   ‚úì Token usage (prompt, completion, total)\")\n",
    "print(\"   ‚úì Timing information (latency)\")\n",
    "print(\"   ‚úì All captured automatically!\")\n",
    "print(\"\\nüîó View trace in MLflow UI: http://localhost:5000\")\n",
    "print(\"   Navigate to: Traces tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ What Just Happened?\n",
    "\n",
    "With `mlflow.openai.autolog()`, MLflow automatically:\n",
    "\n",
    "1. **Intercepted** the OpenAI API call\n",
    "2. **Created** a trace with a unique ID\n",
    "3. **Captured** all inputs (messages, model, parameters)\n",
    "4. **Captured** all outputs (response, tokens, timing)\n",
    "5. **Stored** everything in a structured format\n",
    "6. **Made it available** in the MLflow UI\n",
    "\n",
    "**Single line of code change required!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Tracing Multiple Sequential Calls\n",
    "\n",
    "Let's see how tracing helps with multi-step workflows. That is, multiple sequential call, each creating a trace of its own. This would be typical agentic workflow where an agent might make a series of sequentail calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Multi-step workflow with automatic tracing...\n",
      "\n",
      "Step 1: Generating topic...\n",
      "  Topic: Topic: \"Generative AI as a Co‚ÄëCreator ‚Äî Practical Workflows for Writers, Designers, and Musicians\"\n",
      "\n",
      "Why it‚Äôs interesting:\n",
      "- Generative models (text, image, audio) are moving from novelty to everyday creative tools. Explaining how to use them productively ‚Äî not just to generate raw output but to augment human workflows ‚Äî helps readers get real value while avoiding common pitfalls (garbage outputs, over-reliance, copyright issues).\n",
      "\n",
      "What to cover (short outline):\n",
      "1. Hook: a short scenario showing an AI-human collaboration that speeds a real project.\n",
      "2. Quick primer: what generative models do today (GPT, diffusion, music models) in one paragraph.\n",
      "3. Practical workflows by discipline:\n",
      "   - Writers: idea generation ‚Üí outline ‚Üí draft ‚Üí edit with AI prompts.\n",
      "   - Designers: moodboards ‚Üí rapid mockups ‚Üí iterative refinement.\n",
      "   - Musicians: motif generation ‚Üí arrangement suggestions ‚Üí production templates.\n",
      "4. Prompt engineering tips and quality-control steps.\n",
      "5. Ethics, attribution, and copyright considerations.\n",
      "6. Tools & resources (examples of accessible models, plugins, sample prompts).\n",
      "7. Mini case study or before/after example.\n",
      "8. Actionable takeaway: a 10-minute exercise readers can try.\n",
      "\n",
      "Three catchy post titles:\n",
      "- \"Co‚ÄëCreating with Machines: How to Use Generative AI in Your Creative Workflow\"\n",
      "- \"From Prompt to Polish: Real Workflows for AI‚ÄëAugmented Creativity\"\n",
      "- \"Don‚Äôt Replace, Amplify: Practical Ways to Use AI as a Creative Partner\"\n",
      "\n",
      "Suggested visuals/resources:\n",
      "- A flowchart of a sample workflow (idea ‚Üí AI draft ‚Üí human edit ‚Üí final).\n",
      "- Before/after creative snippets (original vs. AI‚Äëassisted revision).\n",
      "- Links to accessible tools (free tiers of major models, popular plugins).\n",
      "\n",
      "If you want, I can draft a 600‚Äì900 word blog post from that outline or create sample prompts for writers, designers, or musicians. Which would you prefer?\n",
      "\n",
      "Step 2: Creating outline...\n",
      "  Outline: ...\n",
      "\n",
      "Step 3: Writing introduction...\n",
      "  Introduction: Imagine wrapping a client pitch in half a day because an AI sketched a moodboard, generated two narrative arcs, and suggested a chord progression you quickly refined. Generative models for text, images, and audio ‚Äî from GPT and diffusion models to new music generators ‚Äî have moved from novelty to everyday creative tools. This post lays out concise, discipline-specific workflows for writers (idea ‚Üí outline ‚Üí draft ‚Üí edit), designers (moodboard ‚Üí mockup ‚Üí refine), and musicians (motif ‚Üí arrangement ‚Üí production) that treat AI as a co‚Äëcreator, not a replacement. You‚Äôll also find practical prompt tips, quality‚Äëcontrol steps, ethics and copyright guidance, and a 10‚Äëminute exercise to start using AI productively today.\n",
      "\n",
      "‚úÖ All three steps completed!\n",
      "\n",
      "üìä Total tokens used:\n",
      "   2518 tokens\n",
      "\n",
      "üîç View in MLflow UI:\n",
      "   You'll see THREE separate traces, one for each call\n",
      "   Each trace shows timing, tokens, and complete I/O\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-b31a07138c5279e8fd96cd9ecd4d4b0e&amp;experiment_id=2&amp;trace_id=tr-ebd0de11595472a169064368a1fb6d4c&amp;experiment_id=2&amp;trace_id=tr-fee7991e24e578adadbb8e4d914935ef&amp;experiment_id=2&amp;version=3.10.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-b31a07138c5279e8fd96cd9ecd4d4b0e), Trace(trace_id=tr-ebd0de11595472a169064368a1fb6d4c), Trace(trace_id=tr-fee7991e24e578adadbb8e4d914935ef)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple multi-step workflow\n",
    "print(\"\\nüîÑ Multi-step workflow with automatic tracing...\\n\")\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "print(\"Step 1: Generating topic...\")\n",
    "topic_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful AI expert who explains concepts clearly and concisely.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Suggest one interesting AI topic for a blog post.\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "# get the topic\n",
    "topic = topic_response.choices[0].message.content\n",
    "print(f\"  Topic: {topic}\")\n",
    "\n",
    "# Step 2: Generate an outline\n",
    "print(\"\\nStep 2: Creating outline...\")\n",
    "outline_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Create a 3-point outline for a blog post about: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=500\n",
    ")\n",
    "outline = outline_response.choices[0].message.content\n",
    "print(f\"  Outline: {outline[:100]}...\")\n",
    "\n",
    "# Step 3: Write the introduction\n",
    "print(\"\\nStep 3: Writing introduction...\")\n",
    "intro_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Write a 4-sentence introduction paragraph for: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=2000\n",
    ")\n",
    "intro = intro_response.choices[0].message.content\n",
    "print(f\"  Introduction: {intro}\")\n",
    "\n",
    "print(\"\\n‚úÖ All three steps completed!\")\n",
    "print(\"\\nüìä Total tokens used:\")\n",
    "total_tokens = (topic_response.usage.total_tokens +\n",
    "                outline_response.usage.total_tokens +\n",
    "                intro_response.usage.total_tokens)\n",
    "print(f\"   {total_tokens} tokens\")\n",
    "\n",
    "print(\"\\nüîç View in MLflow UI:\")\n",
    "print(\"   You'll see THREE separate traces, one for each call\")\n",
    "print(\"   Each trace shows timing, tokens, and complete I/O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Observation\n",
    "\n",
    "Each OpenAI call creates a **separate trace**. \n",
    "\n",
    "(However, in the next tutorial, we'll learn how to group related calls into a **single hierarchical trace** using manual instrumentation with `@mlflow.trace` decorator.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Automatic Tracing with LangChain (Framework Integration)\n",
    "\n",
    "MLflow supports automatic tracing for many frameworks, over 30+. Let's try a popular one LangChain!\n",
    "\n",
    "LangChain is a Python/JS framework for building LLM-powered applications by providing reusable building blocks for prompts, models, tools/function-calling, retrieval (RAG), memory/state, evalution, and multi-step chains/agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain if needed\n",
    "!pip install langchain langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain autologging enabled\n",
      "‚úÖ Using OpenAI as provider\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from utils.clnt_utils import get_langchain_chat_openai_client, get_databricks_ai_gateway_langchain_client\n",
    "\n",
    "# check if we are running in a Databricks AI Gateway\n",
    "databricks_provider = is_databricks_ai_gateway_client()\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(\"‚úÖ LangChain autologging enabled\")\n",
    "print(\"‚úÖ Using\", \"Databricks AI Gateway\" if databricks_provider else \"OpenAI\", \"as provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Creating and running LangChain chain with tracing...\n",
      "\n",
      "Response: Tracing ‚Äî capturing structured, correlated records of requests, prompts, model responses, intermediate steps, system events and metrics ‚Äî is extremely valuable in generative-AI systems. Key benefits:\n",
      "\n",
      "- Faster debugging and root-cause analysis  \n",
      "  - Correlated traces let you follow a single request from client through prompts, tool calls, model outputs and downstream services, making it far quicker to identify where failures, errors or unexpected outputs occur.\n",
      "\n",
      "- Reproducibility and auditability  \n",
      "  - Storing prompts, model version, temperature, system messages, tool call logs and runtime context lets you reproduce a particular response later for investigation, audits, or regulatory purposes.\n",
      "\n",
      "- Safety and risk detection (hallucinations, bias, policy violations)  \n",
      "  - Traces let you detect patterns of harmful behavior or hallucinations, link them to trigger prompts or model versions, and create mitigation rules or filters.\n",
      "\n",
      "- Performance monitoring and SLO management  \n",
      "  - Trace data provides latency, error rates, and throughput per model, prompt size, or downstream tool, enabling capacity planning and alerting on SLO breaches.\n",
      "\n",
      "- Cost optimization  \n",
      "  - By correlating tokens consumed, model type, and latency with business outcomes, you can choose cheaper models, caching, or prompt techniques where appropriate.\n",
      "\n",
      "- Prompt engineering and product improvement  \n",
      "  - Analyzing which prompts, system messages, or examples lead to desired outputs helps tune prompts, design better templates, and improve UX.\n",
      "\n",
      "- Model evaluation, A/B testing and drift detection  \n",
      "  - Traces let you compare behavior across model versions or experiments, measure regressions, and detect model drift over time from the same input distribution.\n",
      "\n",
      "- Explainability and compliance reporting  \n",
      "  - For regulated domains, trace records provide an evidentiary chain showing why a particular output was produced and which policies or safeguards were applied.\n",
      "\n",
      "- Better telemetry for complex, compositional flows  \n",
      "  - For apps that call external tools, chains of reasoning, or multiple models, tracing lets you see every step in the chain and how they affect final outputs.\n",
      "\n",
      "- Improved incident response and automated mitigation  \n",
      "  - Real-time trace-based alerts can trigger automated rollbacks, throttling, or routing to safer models when risky patterns are detected.\n",
      "\n",
      "Practical tracing items to capture (minimum useful set)\n",
      "- Unique request/session IDs to correlate events  \n",
      "- Timestamped prompt/input, model version, model config (temperature, max tokens)  \n",
      "- Full model output plus confidence/metadata and any intermediate chain-of-thought or tool calls  \n",
      "- Latency and token counts (input/output) and error codes  \n",
      "- Client metadata, session/user id (privacy aware), region, and service/component spans\n",
      "\n",
      "Privacy and compliance cautions\n",
      "- Avoid storing PII unless required and authorized; redact or tokenize sensitive fields  \n",
      "- Use sampling, aggregation, and retention policies to limit exposure and cost  \n",
      "- Encrypt traces and apply least privilege access controls; document retention for audits\n",
      "\n",
      "In short: tracing turns opaque GenAI behavior into observable, actionable data ‚Äî accelerating debugging, improving safety and quality, enabling governance, and helping you optimize performance and cost.\n",
      "\n",
      "‚úÖ LangChain execution traced!\n",
      "\n",
      "üîç In the trace, you'll see:\n",
      "   - Prompt template construction\n",
      "   - Variable substitution\n",
      "   - LLM invocation\n",
      "   - All as separate spans in a hierarchy!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-1244848bf4582de4bcb0f00f4118b8e4&amp;experiment_id=2&amp;version=3.10.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-1244848bf4582de4bcb0f00f4118b8e4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a simple LangChain chain\n",
    "print(\"\\nüîó Creating and running LangChain chain with tracing...\\n\")\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role}. Answer the following question: {question}\"\n",
    ")\n",
    "\n",
    "# Create LangChain LLM object\n",
    "if databricks_provider:\n",
    "    llm = get_databricks_ai_gateway_langchain_client(model_name, temperature=1.0)\n",
    "else:\n",
    "    llm = get_langchain_chat_openai_client(model_name, temperature=1.0)\n",
    "\n",
    "# Create chain using the prompt template and the LLM. \n",
    "# The UNIX like Pipe operator | is used to chain the prompt template and the LLM.\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# Run chain\n",
    "response = chain.invoke({\n",
    "    \"role\": \"helpful AI assistant\",\n",
    "    \"question\": \"What are the benefits of tracing in GenAI applications?\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(\"\\n‚úÖ LangChain execution traced!\")\n",
    "print(\"\\nüîç In the trace, you'll see:\")\n",
    "print(\"   - Prompt template construction\")\n",
    "print(\"   - Variable substitution\")\n",
    "print(\"   - LLM invocation\")\n",
    "print(\"   - All as separate spans in a hierarchy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ LangChain Tracing Benefits\n",
    "\n",
    "With LangChain autologging, MLflow automatically traces:\n",
    "\n",
    "- **Chain execution**: See how data flows through the chain\n",
    "- **Prompt construction**: View how templates are filled\n",
    "- **LLM calls**: Standard OpenAI tracing\n",
    "- **Retrieval steps**: If using RAG components\n",
    "- **Tool usage**: If using agents with tools\n",
    "\n",
    "**All without manual instrumentation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: üö´ Error Tracing and Debugging ü™≤\n",
    "\n",
    "Tracing is especially valuable when things go wrong. And with MLflow 3.9.0+, \n",
    "you can use MLflow Assistant to inpsect, debug, and suggest fixes for you.\n",
    "\n",
    "We will quickly try this here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging with Traces\n",
    "\n",
    "When your LLM application fails:\n",
    "\n",
    "1. **Find the trace** in the UI (filter by error status)\n",
    "2. **Identify the failing span** (marked red)\n",
    "3. **Inspect inputs** that caused the error\n",
    "4. **View error details** (message, type, stack trace)\n",
    "5. **Fix and re-run** with full history preserved\n",
    "6. **Use MLflow Assistant** to inspect and recommend suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Error Scenarios to Debug with MLflow Assistant\n",
    "\n",
    "MLflow captures *failed* traces just as faithfully as successful ones ‚Äî every error, partial result, and stack trace is preserved. Below are three realistic failure patterns that developers commonly hit.\n",
    "\n",
    "**How to use these traces:**\n",
    "1. Run each cell below (errors are expected ‚Äî that's the point!)\n",
    "2. Open the MLflow UI ‚Üí **Traces** tab ‚Üí filter by `Status = Error`\n",
    "3. Click any failed trace, then click **\"Ask MLflow Assistant\"** \n",
    "4. Ask: *\"What caused this error and how do I fix it?\"*\n",
    "\n",
    "| # | Error type | Root cause |\n",
    "|---|-----------|-----------|\n",
    "| 1 | API request error | Invalid role in the message list |\n",
    "| 2 | API request error | Invalid tool/function name (spaces not allowed) |\n",
    "| 3 | Multi-step pipeline | Application bug at step 3 ‚Äî steps 1 & 2 succeed, step 3 crashes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üêõ Error Scenario 1: Invalid Message Role\n",
      "============================================================\n",
      "Calling the API with role='robot' (not a valid OpenAI role)...\n",
      "\n",
      "‚ùå BadRequestError: Error code: 400 - {'error': {'message': \"Invalid value: 'robot'. Supported values are: 'system', 'assistant', 'user', 'function', 'tool', and 'developer'.\", 'type': 'invalid_request_error', 'param': '\n",
      "\n",
      "‚úÖ Failed trace captured ‚Äî open MLflow UI to inspect it\n",
      "   Ask MLflow Assistant: 'Why did this request fail and how do I fix it?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-cb67fa5aef1d9e89b55bf664331c6bdf&amp;experiment_id=2&amp;version=3.10.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-cb67fa5aef1d9e89b55bf664331c6bdf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Error Scenario 1: Invalid Role in the Message List ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Valid OpenAI roles are: \"system\", \"user\", \"assistant\", \"tool\".\n",
    "# Passing an unknown role (\"robot\") triggers a BadRequestError.\n",
    "# Note: Databricks AI Gateway may surface a slightly different error message.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üêõ Error Scenario 1: Invalid Message Role\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Calling the API with role='robot' (not a valid OpenAI role)...\\n\")\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"robot\", \"content\": \"Classify this review as positive or negative.\"},\n",
    "            {\"role\": \"user\",  \"content\": \"I love this product!\"},\n",
    "        ],\n",
    "        max_completion_tokens=50,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Failed trace captured ‚Äî open MLflow UI to inspect it\")\n",
    "print(\"   Ask MLflow Assistant: 'Why did this request fail and how do I fix it?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üêõ Error Scenario 2: Invalid Tool/Function Name\n",
      "============================================================\n",
      "Calling the API with a function name that contains spaces...\n",
      "\n",
      "‚ùå BadRequestError: Error code: 400 - {'error': {'message': \"Invalid 'tools[0].function.name': string does not match pattern. Expected a string that matches the pattern '^[a-zA-Z0-9_-]+$'.\", 'type': 'invalid_request_erro\n",
      "\n",
      "‚úÖ Failed trace captured ‚Äî open MLflow UI to inspect it\n",
      "   Ask MLflow Assistant: 'What is wrong with this tool definition?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-822eb30c32223b6d6e3d583cf2b801cf&amp;experiment_id=2&amp;version=3.10.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-822eb30c32223b6d6e3d583cf2b801cf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Error Scenario 2: Invalid Tool/Function Name ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# OpenAI function names must match ^[a-zA-Z0-9_-]{1,64}$.\n",
    "# Using spaces (e.g. \"get weather data\") is a common mistake that triggers\n",
    "# a BadRequestError ‚Äî captured automatically by mlflow.openai.autolog().\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üêõ Error Scenario 2: Invalid Tool/Function Name\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Calling the API with a function name that contains spaces...\\n\")\n",
    "\n",
    "# Common mistake: using a natural-language name with spaces instead of\n",
    "# snake_case or kebab-case (e.g. 'get_weather' or 'get-weather').\n",
    "broken_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get weather data\",   # ‚Üê BUG: spaces are not allowed\n",
    "                                         # Fix: use 'get_weather_data'\n",
    "        \"description\": \"Retrieve the current weather for a given city.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "            },\n",
    "            \"required\": [\"city\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the weather in Paris?\"}],\n",
    "        tools=[broken_tool],\n",
    "        max_completion_tokens=100,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Failed trace captured ‚Äî open MLflow UI to inspect it\")\n",
    "print(\"   Ask MLflow Assistant: 'What is wrong with this tool definition?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üêõ Error Scenario 3: Multi-Step Pipeline ‚Äî Step 3 Fails\n",
      "============================================================\n",
      "3-step pipeline: steps 1 & 2 succeed, step 3 crashes...\n",
      "\n",
      "‚ùå Pipeline failed at Step 3 (ValueError): invalid literal for int() with base 10: '8/10'\n",
      "\n",
      "üí° Steps 1 & 2 succeeded ‚Äî their outputs are preserved in the trace\n",
      "\n",
      "‚úÖ Hierarchical trace captured ‚Äî open MLflow UI to inspect it\n",
      "   You will see:\n",
      "   ‚úì Step 1 span: product review generated  (Success)\n",
      "   ‚úì Step 2 span: LLM returned rating text  (Success)\n",
      "   ‚úó Step 3 span: integer parsing failed    (Error)\n",
      "\n",
      "   Ask MLflow Assistant: 'Which step failed and what did each step return?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-8d8178e2be629404dd1f511803edb974&amp;experiment_id=2&amp;version=3.10.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-8d8178e2be629404dd1f511803edb974)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Error Scenario 3: Multi-Step Pipeline ‚Äî Step 3 Fails ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# A three-step LangChain LCEL pipeline:\n",
    "#   Step 1 ‚Äì Generate a product review           (succeeds)\n",
    "#   Step 2 ‚Äì Ask the LLM to rate the review      (succeeds, returns \"X/10\")\n",
    "#   Step 3 ‚Äì Parse the rating as a bare integer  (FAILS ‚Äì \"8/10\" ‚Üí ValueError)\n",
    "#\n",
    "# mlflow.langchain.autolog() (enabled earlier) records the ENTIRE pipeline as\n",
    "# ONE hierarchical trace, so you can see each step's output even after the\n",
    "# pipeline fails ‚Äî no @mlflow.trace or manual instrumentation needed.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üêõ Error Scenario 3: Multi-Step Pipeline ‚Äî Step 3 Fails\")\n",
    "print(\"=\" * 60)\n",
    "print(\"3-step pipeline: steps 1 & 2 succeed, step 3 crashes...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "parser = StrOutputParser()\n",
    "product = \"ultra-lightweight noise-cancelling wireless headphones\"\n",
    "\n",
    "def parse_integer_rating(text: str) -> int:\n",
    "    \"\"\"BUG: LLM returns 'X/10' format, but int() cannot parse it.\"\"\"\n",
    "    return int(text.strip())  # ‚Üê raises ValueError when text is e.g. \"8/10\"\n",
    "\n",
    "# Build the full LCEL pipeline.\n",
    "# mlflow.langchain.autolog() captures it as a SINGLE hierarchical trace\n",
    "# with one span per step ‚Äî you will see the successful spans for steps 1 & 2\n",
    "# even though step 3 fails.\n",
    "pipeline = (\n",
    "    # Step 1: generate a 2-sentence product review (succeeds)\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"Write a 2-sentence product review for: {product}\"\n",
    "    )\n",
    "    | llm\n",
    "    | parser\n",
    "    # Step 2: rate the review ‚Äî explicitly ask for \"X/10\" format (succeeds)\n",
    "    | RunnableLambda(lambda review: {\"review\": review})\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Rate this product review from 1 to 10. Reply in the format 'X/10':\\n\\n{review}\"\n",
    "    )\n",
    "    | llm\n",
    "    | parser\n",
    "    # Step 3: parse the rating as a plain integer (FAILS)\n",
    "    | RunnableLambda(parse_integer_rating)  # ‚Üê \"8/10\" ‚Üí ValueError\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = pipeline.invoke({\"product\": product})\n",
    "    print(f\"Rating: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline failed at Step 3 ({type(e).__name__}): {e}\")\n",
    "    print(\"\\nüí° Steps 1 & 2 succeeded ‚Äî their outputs are preserved in the trace\")\n",
    "\n",
    "print(\"\\n‚úÖ Hierarchical trace captured ‚Äî open MLflow UI to inspect it\")\n",
    "print(\"   You will see:\")\n",
    "print(\"   ‚úì Step 1 span: product review generated  (Success)\")\n",
    "print(\"   ‚úì Step 2 span: LLM returned rating text  (Success)\")\n",
    "print(\"   ‚úó Step 3 span: integer parsing failed    (Error)\")\n",
    "print(\"\\n   Ask MLflow Assistant: 'Which step failed and what did each step return?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLflow Assistant to Debug These Traces\n",
    "\n",
    "After running the cells above, all three failures are visible in the MLflow UI as **Error** traces.\n",
    "\n",
    "**Step-by-step:**\n",
    "1. Go to **http://localhost:5000** ‚Üí **Traces** tab\n",
    "2. Use the **Status** filter ‚Üí select **Error**\n",
    "3. Click on a trace to expand it\n",
    "4. For hierarchical traces (Scenarios 3 & 4), click the **red span** to see exactly where it failed\n",
    "5. Click **\"Ask Claude\"** and try prompts like:\n",
    "   - *\"What caused this error and how do I fix it?\"*\n",
    "   - *\"Which span failed and what was its input?\"*\n",
    "   - *\"Show me the corrected code\"*\n",
    "\n",
    "> **Key insight:** Even when a multi-step pipeline fails mid-way, MLflow preserves all the\n",
    "> *successful* upstream spans ‚Äî so you can see exactly what data each step received and\n",
    "> produced before the failure occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Best Practices for Tracing\n",
    "\n",
    "Let's review key best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë           Tracing Best Practices                             ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "‚úÖ DO:\n",
      "\n",
      "1. Enable autologging early in development\n",
      "   - Easier to debug from the start\n",
      "   - Understand performance characteristics early\n",
      "\n",
      "2. Use tracing in all environments\n",
      "   - Development: Full tracing for debugging\n",
      "   - Staging: Trace all requests\n",
      "   - Production: Sample or trace all (depending on volume)\n",
      "\n",
      "3. Combine tracing with experiment tracking\n",
      "   - Traces give you the \"how\"\n",
      "   - Experiments give you the \"what\" and \"why\"\n",
      "\n",
      "4. Review traces regularly\n",
      "   - Look for performance bottlenecks\n",
      "   - Identify expensive operations\n",
      "   - Understand failure patterns\n",
      "\n",
      "5. Use multiple frameworks\n",
      "   - MLflow supports 30+ integrations\n",
      "   - Pick the best tool for each task\n",
      "\n",
      "‚ùå DON'T:\n",
      "\n",
      "1. Ignore traces until there's a problem\n",
      "   - Proactive monitoring prevents issues\n",
      "\n",
      "2. Log sensitive data in traces\n",
      "   - PII, credentials, confidential info\n",
      "   - Use data masking if needed\n",
      "\n",
      "3. Assume autologging captures everything\n",
      "   - Custom operations need manual instrumentation\n",
      "   - We'll cover this in the next notebook!\n",
      "\n",
      "4. Over-rely on individual traces\n",
      "   - Look at aggregate patterns\n",
      "   - Use traces + metrics together\n",
      "\n",
      "üí° PRO TIPS:\n",
      "\n",
      "- Set up trace sampling in high-volume production\n",
      "- Archive old traces to manage storage\n",
      "- Use MLflow UI to inspect traces and metrics\n",
      "- Use Claude Code Assistant to inspect, debug, and suggest fixes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë           Tracing Best Practices                             ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "‚úÖ DO:\n",
    "\n",
    "1. Enable autologging early in development\n",
    "   - Easier to debug from the start\n",
    "   - Understand performance characteristics early\n",
    "\n",
    "2. Use tracing in all environments\n",
    "   - Development: Full tracing for debugging\n",
    "   - Staging: Trace all requests\n",
    "   - Production: Sample or trace all (depending on volume)\n",
    "\n",
    "3. Combine tracing with experiment tracking\n",
    "   - Traces give you the \"how\"\n",
    "   - Experiments give you the \"what\" and \"why\"\n",
    "\n",
    "4. Review traces regularly\n",
    "   - Look for performance bottlenecks\n",
    "   - Identify expensive operations\n",
    "   - Understand failure patterns\n",
    "\n",
    "5. Use multiple frameworks\n",
    "   - MLflow supports 30+ integrations\n",
    "   - Pick the best tool for each task\n",
    "\n",
    "‚ùå DON'T:\n",
    "\n",
    "1. Ignore traces until there's a problem\n",
    "   - Proactive monitoring prevents issues\n",
    "\n",
    "2. Log sensitive data in traces\n",
    "   - PII, credentials, confidential info\n",
    "   - Use data masking if needed\n",
    "\n",
    "3. Assume autologging captures everything\n",
    "   - Custom operations need manual instrumentation\n",
    "   - We'll cover this in the next notebook!\n",
    "\n",
    "4. Over-rely on individual traces\n",
    "   - Look at aggregate patterns\n",
    "   - Use traces + metrics together\n",
    "\n",
    "üí° PRO TIPS:\n",
    "\n",
    "- Set up trace sampling in high-volume production\n",
    "- Archive old traces to manage storage\n",
    "- Use MLflow UI to inspect traces and metrics\n",
    "- Use Claude Code Assistant to inspect, debug, and suggest fixes\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. What tracing is and why it's critical for GenAI\n",
    "2. The trace data model (traces, spans, hierarchy)\n",
    "3. Automatic tracing with `mlflow.openai.autolog()`\n",
    "4. How to view and analyze traces in the MLflow UI\n",
    "5. Tracing multi-step workflows\n",
    "6. Framework integration (LangChain example)\n",
    "7. Error tracing and debugging with MLflow Assistant\n",
    "8. Best practices for production use\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **One line of code** enables complete observability\n",
    "- **Automatic capture** of inputs, outputs, and metadata\n",
    "- **Framework agnostic** - works with OpenAI, LangChain, LlamaIndex, etc.\n",
    "- **Essential for debugging** complex LLM workflows\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Notebook 1.4: Manual Tracing and Advanced Observability**\n",
    "\n",
    "Learn how to:\n",
    "- Create custom spans with `@mlflow.trace`\n",
    "- Add custom attributes to spans\n",
    "- Build hierarchical traces for complex workflows\n",
    "- Trace RAG pipelines with retrieval and generation steps\n",
    "- Trace agentic workflows with tool usage\n",
    "- Debug production issues with advanced techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
