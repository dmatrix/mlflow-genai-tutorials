{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.3: Introduction to Tracing\n",
    "\n",
    "![](images/4_Notebook-13-Introduction-to-Tracing.png)\n",
    "\n",
    "## LLM Observability for GenAI Applications\n",
    "\n",
    "Welcome to the tracing section! This is where MLflow really shines for GenAI development. You'll learn how to gain complete visibility into your LLM applications.\n",
    "\n",
    "### What You'll Learn\n",
    "- What is tracing and why it matters for GenAI\n",
    "- Understanding the trace data model (traces, spans, hierarchy)\n",
    "- Automatic tracing with autologging\n",
    "- Tracing multiple frameworks (OpenAI, LangChain)\n",
    "- Viewing and analyzing traces in MLflow UI\n",
    "- Debugging with traces uing Claude Code MLflow Assistant\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebooks 1.1 and 1.2\n",
    "- MLflow UI running\n",
    "- OpenAI API key configured\n",
    "- Or keys for your foundational model provider, e.g., Databricks\n",
    "\n",
    "### Estimated Time: 20-25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: What is Tracing?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional experimenting tracking or logging isn't enough for LLM applications:\n",
    "\n",
    "```python\n",
    "# Traditional logging - hard to debug\n",
    "print(\"Calling LLM...\")\n",
    "response = llm.generate(prompt)\n",
    "print(f\"Response: {response}\")\n",
    "# What happened inside? How long did it take? What was sent or received?\n",
    "```\n",
    "\n",
    "### The Solution: Distributed Tracing\n",
    "\n",
    "Tracing captures the **complete execution flow** of your application:\n",
    "\n",
    "```\n",
    "Trace: RAG Application\n",
    "â”œâ”€â”€ Span 1: Embed Query [0ms - 200ms]\n",
    "â”‚   Input: \"What is MLflow for GenAI?\"\n",
    "â”‚   Output: [0.123, 0.456, ...]\n",
    "â”‚   \n",
    "â”œâ”€â”€ Span 2: Retrieve Documents [200ms - 350ms]\n",
    "â”‚   Input: [0.123, 0.456, ...]\n",
    "â”‚   Output: [\"MLflow is...\", \"The platform...\"]\n",
    "â”‚   \n",
    "â””â”€â”€ Span 3: Generate Response [350ms - 1500ms]\n",
    "    Input: {query, documents}\n",
    "    Output: \"MLflow is an open source platform...\"\n",
    "    LLM: gpt-5-mini\n",
    "    Tokens: 150\n",
    "```\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Visibility**: See every step in your LLM workflow\n",
    "2. **Performance**: Identify bottlenecks and latency issues\n",
    "3. **Debugging**: Trace errors to their exact source\n",
    "4. **Cost**: Track token usage per operation\n",
    "5. **Quality**: Inspect inputs/outputs at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding the Trace Data Model\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Trace**: A complete execution of an operation\n",
    "- Represents one request or workflow\n",
    "- Contains one or more spans\n",
    "- Has a root span\n",
    "\n",
    "**Span**: A single operation within a trace\n",
    "- Has a start and end time\n",
    "- Contains inputs and outputs\n",
    "- Has metadata (model, tokens,latecy, etc.)\n",
    "- Can have parent-child relationships\n",
    "\n",
    "**Span Attributes**: Additional metadata\n",
    "- Model name\n",
    "- Token counts\n",
    "- Temperature\n",
    "- Custom attributes\n",
    "\n",
    "### Span Types\n",
    "\n",
    "MLflow defines standard span types:\n",
    "\n",
    "- `CHAIN`: A sequence of operations\n",
    "- `LLM`: Language model call\n",
    "- `RETRIEVER`: Document retrieval\n",
    "- `EMBEDDING`: Text embedding\n",
    "- `TOOL`: Tool/function execution\n",
    "- `AGENT`: Agent reasoning\n",
    "- `PARSER`: Output parsing or generic intermediate parsing of an outcome \n",
    "\n",
    "### Hierarchy Example\n",
    "\n",
    "```\n",
    "TRACE (root)\n",
    "â”‚\n",
    "â””â”€ SPAN: Agent Executor (AGENT)\n",
    "   â”‚\n",
    "   â”œâ”€ SPAN: Planning Step (LLM)\n",
    "   â”‚  â””â”€ attributes: {model: gpt-5, tokens: 50}\n",
    "   â”‚\n",
    "   â”œâ”€ SPAN: Tool Execution (TOOL)\n",
    "   â”‚  â””â”€ attributes: {tool: search, query: \"...\"}\n",
    "   â”‚\n",
    "   â””â”€ SPAN: Final Response (LLM)\n",
    "      â””â”€ attributes: {model: gpt-5, tokens: 150}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured: using OpenAI client\n",
      "   MLflow version: 3.10.0rc0\n",
      "   Tracking URI: http://localhost:5000\n",
      "   Using model: gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "else:\n",
    "    # Initialize OpenAI\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-mini\"\n",
    "\n",
    "print(\"âœ… Environment configured: using\", \"Databricks\" if use_databricks_provider else \"OpenAI\", \"client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Using model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Your First Trace - Manual Example\n",
    "\n",
    "Before we use autologging, let's understand what tracing captures by without autologging the information. This way we understand the difference and benefits of using autlogging. To some extent, we saw the benefits of autologgin in the previous notebook on experimental tracking LLM calls. So this is just a quick reminder in case you missed that tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/18 17:25:44 INFO mlflow.tracking.fluent: Experiment with name '06-tracing-introduction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Making LLM call WITHOUT tracing...\n",
      "\n",
      "Response: Distributed tracing is a technique for recording and correlating timed operations (spans) across multiple services to reconstruct and analyze end-to-end request flows in distributed systems.\n",
      "\n",
      "âŒ What we DON'T know:\n",
      "   - Exact timing of the call\n",
      "   - Detailed request/response structure\n",
      "   - Easy way to correlate with other operations\n",
      "   - Visual representation of execution\n",
      "\n",
      "ğŸ” View trace in MLflow UI: http://localhost:5000\n",
      "   Navigate to: Traces tab\n"
     ]
    }
   ],
   "source": [
    "# Create experiment for tracing examples\n",
    "mlflow.set_experiment(\"06-tracing-introduction\")\n",
    "\n",
    "# Without tracing - basic call\n",
    "print(\"\\nğŸ“ Making LLM call WITHOUT tracing...\\n\")\n",
    "\n",
    "prompt = \"Explain what distributed tracing is in one sentence.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\nâŒ What we DON'T know:\")\n",
    "print(\"   - Exact timing of the call\")\n",
    "print(\"   - Detailed request/response structure\")\n",
    "print(\"   - Easy way to correlate with other operations\")\n",
    "print(\"   - Visual representation of execution\")\n",
    "\n",
    "# check the trace in the UI\n",
    "print(\"\\nğŸ” View trace in MLflow UI: http://localhost:5000\")\n",
    "print(\"   Navigate to: Traces tab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Automatic Tracing with OpenAI Autologging\n",
    "\n",
    "Now let's enable tracing with a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI autologging enabled\n",
      "   All OpenAI API calls will now be automatically traced!\n"
     ]
    }
   ],
   "source": [
    "# Enable OpenAI autologging - THIS IS THE MAGIC LINE!\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"âœ… OpenAI autologging enabled\")\n",
    "print(\"   All OpenAI API calls will now be automatically traced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Making LLM call WITH tracing...\n",
      "\n",
      "Response: Distributed tracing is a technique for tracking and visualizing the path and timing of an individual request as it flows through multiple services or components in a distributed system by recording and correlating timed \"spans\" for each operation.\n",
      "\n",
      "âœ… What we NOW know:\n",
      "   âœ“ Complete request details (model, messages, parameters)\n",
      "   âœ“ Response content and metadata\n",
      "   âœ“ Token usage (prompt, completion, total)\n",
      "   âœ“ Timing information (latency)\n",
      "   âœ“ All captured automatically!\n",
      "\n",
      "ğŸ”— View trace in MLflow UI: http://localhost:5000\n",
      "   Navigate to: Traces tab\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-4919148cf628cd8a9bb640baf35aea54&amp;experiment_id=1&amp;version=3.10.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-4919148cf628cd8a9bb640baf35aea54)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the same call - now it's automatically traced!\n",
    "print(\"\\nğŸ” Making LLM call WITH tracing...\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\nâœ… What we NOW know:\")\n",
    "print(\"   âœ“ Complete request details (model, messages, parameters)\")\n",
    "print(\"   âœ“ Response content and metadata\")\n",
    "print(\"   âœ“ Token usage (prompt, completion, total)\")\n",
    "print(\"   âœ“ Timing information (latency)\")\n",
    "print(\"   âœ“ All captured automatically!\")\n",
    "print(\"\\nğŸ”— View trace in MLflow UI: http://localhost:5000\")\n",
    "print(\"   Navigate to: Traces tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‰ What Just Happened?\n",
    "\n",
    "With `mlflow.openai.autolog()`, MLflow automatically:\n",
    "\n",
    "1. **Intercepted** the OpenAI API call\n",
    "2. **Created** a trace with a unique ID\n",
    "3. **Captured** all inputs (messages, model, parameters)\n",
    "4. **Captured** all outputs (response, tokens, timing)\n",
    "5. **Stored** everything in a structured format\n",
    "6. **Made it available** in the MLflow UI\n",
    "\n",
    "**Single line of code change required!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Tracing Multiple Sequential Calls\n",
    "\n",
    "Let's see how tracing helps with multi-step workflows. That is, multiple sequential call, each creating a trace of its own. This would be typical agentic workflow where an agent might make a series of sequentail calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Multi-step workflow with automatic tracing...\n",
      "\n",
      "Step 1: Generating topic...\n",
      "  Topic: Topic: \"Tool-Using AI Agents â€” How LLMs Become Task-Oriented Helpers\"\n",
      "\n",
      "One-sentence hook: Explore how large language models are being combined with external tools (search, code execution, APIs, browsers, sensors) and orchestration to form autonomous agents that can perform real-world tasks, and what that means for usefulness, reliability, and safety.\n",
      "\n",
      "Why itâ€™s interesting:\n",
      "- Shows a concrete path from conversational models to practical automation (booking, data analysis, coding, research).\n",
      "- Raises current technical questions (planning, tool selection, chaining, grounding) and evaluation challenges.\n",
      "- Touches on important safety, privacy, and trust trade-offs as models gain ability to act.\n",
      "- Lots of accessible demos and code repos to illustrate concepts for readers.\n",
      "\n",
      "Suggested short outline for the post:\n",
      "1. Opening example: a single prompt that triggers an agent to search the web, run a script, and schedule a meeting.\n",
      "2. Core components: planner (reasoning), tool interface (APIs, executors), memory/state, and verifier (to check outputs).\n",
      "3. How tool use reduces hallucination and extends capabilities â€” plus limits and failure modes.\n",
      "4. Demo ideas: building a mini agent that uses a web search and a Python REPL to answer data questions.\n",
      "5. Ethical and safety considerations: permissions, privacy, malicious automation, auditability.\n",
      "6. Resources and next steps: key papers (ReAct, Toolformer, AutoGPT-style projects), libraries, and example repos.\n",
      "\n",
      "If you want, I can draft the full blog post or a short demo notebook for the mini agent. Which would you prefer?\n",
      "\n",
      "Step 2: Creating outline...\n",
      "  Outline: ...\n",
      "\n",
      "Step 3: Writing introduction...\n",
      "  Introduction: Large language models are no longer just conversationalists â€” when combined with external tools (search, code execution, APIs, browsers, sensors) and lightweight orchestration, they can act as autonomous agents that perform real-world tasks. This shift maps a concrete path from chatty assistants to practical automation â€” booking travel, analyzing data, writing and running code, or conducting research â€” by enabling models to plan, select and chain tools, and maintain state. At the same time, tool use surfaces hard technical questions (planning, tool selection, chaining, grounding), evaluation challenges, and pressing safety, privacy, and trust trade-offs as models gain the ability to act. Iâ€™ll outline a compact roadmap â€” an example prompt that triggers multi-tool behavior, the core components (planner, tool interface, memory, verifier), demo ideas, and ethical considerations â€” and I can follow up with either a full blog post or a short demo notebook; which would you prefer?\n",
      "\n",
      "âœ… All three steps completed!\n",
      "\n",
      "ğŸ“Š Total tokens used:\n",
      "   2393 tokens\n",
      "\n",
      "ğŸ” View in MLflow UI:\n",
      "   You'll see THREE separate traces, one for each call\n",
      "   Each trace shows timing, tokens, and complete I/O\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-d6fa8cbc53a47322396496f4a0e0d783&amp;experiment_id=1&amp;trace_id=tr-cb6b647607a3166fddc2243f443b8305&amp;experiment_id=1&amp;trace_id=tr-5a8fab2b15557523da75ff995425b5d1&amp;experiment_id=1&amp;version=3.10.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-d6fa8cbc53a47322396496f4a0e0d783), Trace(trace_id=tr-cb6b647607a3166fddc2243f443b8305), Trace(trace_id=tr-5a8fab2b15557523da75ff995425b5d1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple multi-step workflow\n",
    "print(\"\\nğŸ”„ Multi-step workflow with automatic tracing...\\n\")\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "print(\"Step 1: Generating topic...\")\n",
    "topic_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful AI expert who explains concepts clearly and concisely.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Suggest one interesting AI topic for a blog post.\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "# get the topic\n",
    "topic = topic_response.choices[0].message.content\n",
    "print(f\"  Topic: {topic}\")\n",
    "\n",
    "# Step 2: Generate an outline\n",
    "print(\"\\nStep 2: Creating outline...\")\n",
    "outline_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Create a 3-point outline for a blog post about: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=500\n",
    ")\n",
    "outline = outline_response.choices[0].message.content\n",
    "print(f\"  Outline: {outline[:100]}...\")\n",
    "\n",
    "# Step 3: Write the introduction\n",
    "print(\"\\nStep 3: Writing introduction...\")\n",
    "intro_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Write a 4-sentence introduction paragraph for: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_completion_tokens=2000\n",
    ")\n",
    "intro = intro_response.choices[0].message.content\n",
    "print(f\"  Introduction: {intro}\")\n",
    "\n",
    "print(\"\\nâœ… All three steps completed!\")\n",
    "print(\"\\nğŸ“Š Total tokens used:\")\n",
    "total_tokens = (topic_response.usage.total_tokens +\n",
    "                outline_response.usage.total_tokens +\n",
    "                intro_response.usage.total_tokens)\n",
    "print(f\"   {total_tokens} tokens\")\n",
    "\n",
    "print(\"\\nğŸ” View in MLflow UI:\")\n",
    "print(\"   You'll see THREE separate traces, one for each call\")\n",
    "print(\"   Each trace shows timing, tokens, and complete I/O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Observation\n",
    "\n",
    "Each OpenAI call creates a **separate trace**. \n",
    "\n",
    "(In the next notebook, we'll learn how to group related calls into a **single hierarchical trace** using manual instrumentation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Tracing with LangChain (Framework Integration)\n",
    "\n",
    "MLflow supports automatic tracing for many frameworks, over 30+ Let's try a popular one LangChain!\n",
    "\n",
    "LangChain is a Python/JS framework for building LLM-powered applications by providing reusable building blocks for prompts, models, tools/function-calling, retrieval (RAG), memory/state, evalution, and multi-step chains/agents.\n",
    "\n",
    "Why/when to use it\n",
    "Use LangChain when you want to:\n",
    "\n",
    "**Standardize an LLM app**: consistent interfaces for prompt templates, chat models (e.g., ChatOpenAI), outputs/parsers, retries, and tracing.\n",
    "\n",
    "**Build RAG**: connect documents â†’ embeddings â†’ vector store â†’ retriever â†’ prompt, with common patterns already implemented.\n",
    "\n",
    "**Use tools/agents**: let the model call functions/tools and orchestrate multi-step workflows.\n",
    "\n",
    "**Compose pipelines**: easily chain steps (prompt â†’ model â†’ parser) and swap components without rewriting glue code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain if needed\n",
    "!pip install langchain langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain autologging enabled\n",
      "âœ… Using OpenAI as provider\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from utils.clnt_utils import get_langchain_chat_openai_client, get_databricks_ai_gateway_langchain_client\n",
    "\n",
    "databricks_provider = is_databricks_ai_gateway_client()\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(\"âœ… LangChain autologging enabled\")\n",
    "print(\"âœ… Using\", \"Databricks AI Gateway\" if databricks_provider else \"OpenAI\", \"as provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— Creating and running LangChain chain with tracing...\n",
      "\n",
      "Response: Short answer\n",
      "Tracing â€” recording structured, correlated data about prompts, inputs, model calls, outputs, tool actions, retrievals, and timing â€” gives GenAI teams the visibility they need to operate, debug, govern and improve systems safely and efficiently.\n",
      "\n",
      "Why tracing matters (key benefits)\n",
      "- Faster debugging and root-cause analysis\n",
      "  - See the exact prompt, model parameters, token stream, and intermediate tool calls that produced a bad output so you can reproduce and fix it.\n",
      "- Reproducibility and experimentability\n",
      "  - Versioned traces (model version, seed, config) let you re-run and compare experiments reliably.\n",
      "- Observability and performance optimization\n",
      "  - Latency, token counts and resource metrics let you spot bottlenecks, optimize prompt length, batching and cost.\n",
      "- Provenance and hallucination mitigation\n",
      "  - RAG/source traces show which documents or snippets the model used, making it easier to verify answers and detect fabricated claims.\n",
      "- Safety, compliance and auditability\n",
      "  - Immutable traces provide evidence for regulatory audits, safety reviews, and content moderation investigations.\n",
      "- Drift detection and model quality monitoring\n",
      "  - Track trends in errors, user corrections, confidence, and retrieval relevance to detect model/data drift early.\n",
      "- Cost allocation and optimization\n",
      "  - Correlate calls, tokens, and latencies with features, customers or teams to manage spend and optimize pricing.\n",
      "- Product analytics and UX improvement\n",
      "  - Understand how users prompt, how many retries they do, where the system fails, and tune UX, prompts and tooling.\n",
      "- Accountability and trust\n",
      "  - Providing provenance, traces and redacted logs builds user and stakeholder trust in outputs and decisions.\n",
      "- Forensics and security\n",
      "  - Traces help investigate misuse, abuse, or data exfiltration incidents and support access-control audits.\n",
      "\n",
      "What to capture in a trace (recommended fields)\n",
      "- Request metadata: timestamp, request id, correlated trace id, user id (hashed/pseudonymized), app/feature id\n",
      "- Prompt/input: original prompt and normalized prompt (with sensitive data redacted)\n",
      "- Model call details: model name/version, parameters (temperature, max tokens), random seed if used\n",
      "- Token-level info: token counts, token latencies, probabilities/confidences (if available)\n",
      "- Outputs: generated text, structured outputs, tool results\n",
      "- Retrieval/RAG evidence: source ids, snippets, document metadata, retrieval scores\n",
      "- Tool/chain actions: tool calls, API calls, external DB queries and responses\n",
      "- Performance: latency, throughput, resource usage, error codes\n",
      "- Labels/feedback: human review, user corrections, automated evaluation scores\n",
      "- Environment/context: deployment instance, container/pod id, model weights hash\n",
      "\n",
      "Best practices and governance\n",
      "- Minimize PII: redact, hash, or avoid storing sensitive user data; use explicit consent where required.\n",
      "- Sampling and retention: sample high-volume traffic, keep high-fidelity traces for important flows and shorten retention for routine logs.\n",
      "- Access controls & encryption: protect traces at rest and in transit; audit who can view them.\n",
      "- Correlate logs, metrics and traces: use a tracing standard (OpenTelemetry, W3C Trace Context) to tie traces to metrics and logs.\n",
      "- Immutable, versioned traces: store model version and prompt histories so results are reproducible.\n",
      "- Cost/volume management: compress token-level traces, log full token streams only when necessary (errors, audits).\n",
      "- Privacy & compliance: follow GDPR/CCPA rules for storage, right-to-be-forgotten requests; document policies.\n",
      "\n",
      "Challenges and trade-offs\n",
      "- Storage and cost: full token- and tool-level traces are large; use sampling and retention strategies.\n",
      "- Performance overhead: tracing adds latency/compute; keep tracing async where possible.\n",
      "- Privacy risk: capturing prompts may expose PII â€” require rigorous redaction and policies.\n",
      "- Complexity: need correlation across distributed components, and tooling to analyze traces meaningfully.\n",
      "\n",
      "Tooling and standards (examples)\n",
      "- Use OpenTelemetry or vendor tracing (Datadog, New Relic) for distributed traces.\n",
      "- Integrate model/chain tracing in libraries (e.g., LangChain tracing features, LlamaIndex provenance).\n",
      "- Store provenance for retrieval (document IDs, offsets) to support explainability.\n",
      "\n",
      "Short example trace event (illustrative)\n",
      "- trace_id, request_id, timestamp, user_hash, feature=\"answer_question\", prompt_redacted, model=\"gpt-4o-2026-01\", temp=0.2, max_tokens=512, token_count=243, latency_ms=342, sources=[doc123:score0.91, doc456:score0.57], output_text, human_label=\"incorrect\", error=null\n",
      "\n",
      "Bottom line\n",
      "Tracing is essential infrastructure for safe, reliable, debuggable, and explainable GenAI systems. It enables faster fixes, better models, provable provenance, cost control, and regulatory compliance â€” if implemented thoughtfully with privacy, cost and performance trade-offs managed.\n",
      "\n",
      "âœ… LangChain execution traced!\n",
      "\n",
      "ğŸ” In the trace, you'll see:\n",
      "   - Prompt template construction\n",
      "   - Variable substitution\n",
      "   - LLM invocation\n",
      "   - All as separate spans in a hierarchy!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-b7136987e2666f22357d6c4c8b2eb44e&amp;experiment_id=1&amp;version=3.10.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-b7136987e2666f22357d6c4c8b2eb44e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a simple LangChain chain\n",
    "print(\"\\nğŸ”— Creating and running LangChain chain with tracing...\\n\")\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role}. Answer the following question: {question}\"\n",
    ")\n",
    "\n",
    "# Create LLM\n",
    "if databricks_provider:\n",
    "    llm = get_databricks_ai_gateway_langchain_client(model_name, temperature=1.0)\n",
    "else:\n",
    "    llm = get_langchain_chat_openai_client(model_name, temperature=1.0)\n",
    "\n",
    "# Create chain using the prompt template and the LLM. \n",
    "# the UNIX like Pipe operator | is used to chain the prompt template and the LLM.\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# Run chain\n",
    "response = chain.invoke({\n",
    "    \"role\": \"helpful AI assistant\",\n",
    "    \"question\": \"What are the benefits of tracing in GenAI applications?\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(\"\\nâœ… LangChain execution traced!\")\n",
    "print(\"\\nğŸ” In the trace, you'll see:\")\n",
    "print(\"   - Prompt template construction\")\n",
    "print(\"   - Variable substitution\")\n",
    "print(\"   - LLM invocation\")\n",
    "print(\"   - All as separate spans in a hierarchy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ LangChain Tracing Benefits\n",
    "\n",
    "With LangChain autologging, MLflow automatically traces:\n",
    "\n",
    "- **Chain execution**: See how data flows through the chain\n",
    "- **Prompt construction**: View how templates are filled\n",
    "- **LLM calls**: Standard OpenAI tracing\n",
    "- **Retrieval steps**: If using RAG components\n",
    "- **Tool usage**: If using agents with tools\n",
    "\n",
    "**All without manual instrumentation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Error Tracing and Debugging\n",
    "\n",
    "Tracing is especially valuable when things go wrong. And with MLflow 3.9.0+, you can use Claude Code Assistant \n",
    "inpsect, debug, and suggest fixes for you.\n",
    "\n",
    "We will quickly try this here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ› Demonstrating error tracing...\n",
      "\n",
      "âŒ Error occurred: NotFoundError\n",
      "   Message: Error code: 404 - {'error': {'message': 'The model `gpt-does-not-exist` does not exist or you do not...\n",
      "\n",
      "âœ… Error was captured in trace!\n",
      "\n",
      "ğŸ” In the MLflow UI:\n",
      "   - Trace marked with error status\n",
      "   - Error message visible\n",
      "   - Stack trace preserved\n",
      "   - Easy to identify failure point\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-27d14c787ea748e3767a9c44178494e0&amp;experiment_id=1&amp;version=3.10.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-27d14c787ea748e3767a9c44178494e0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intentionally cause an error to see how it's traced\n",
    "print(\"\\nğŸ› Demonstrating error tracing...\\n\")\n",
    "\n",
    "try:\n",
    "    # This will fail - invalid model name\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-does-not-exist\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error occurred: {type(e).__name__}\")\n",
    "    print(f\"   Message: {str(e)[:100]}...\")\n",
    "    print(\"\\nâœ… Error was captured in trace!\")\n",
    "    print(\"\\nğŸ” In the MLflow UI:\")\n",
    "    print(\"   - Trace marked with error status\")\n",
    "    print(\"   - Error message visible\")\n",
    "    print(\"   - Stack trace preserved\")\n",
    "    print(\"   - Easy to identify failure point\")\n",
    "    print(\"   - Claude Code Assistant can inspect, debug, and suggest fixes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging with Traces\n",
    "\n",
    "When your LLM application fails:\n",
    "\n",
    "1. **Find the trace** in the UI (filter by error status)\n",
    "2. **Identify the failing span** (marked red)\n",
    "3. **Inspect inputs** that caused the error\n",
    "4. **View error details** (message, type, stack trace)\n",
    "5. **Fix and re-run** with full history preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Best Practices for Tracing\n",
    "\n",
    "Let's review key best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘           Tracing Best Practices                             â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "âœ… DO:\n",
      "\n",
      "1. Enable autologging early in development\n",
      "   - Easier to debug from the start\n",
      "   - Understand performance characteristics early\n",
      "\n",
      "2. Use tracing in all environments\n",
      "   - Development: Full tracing for debugging\n",
      "   - Staging: Trace all requests\n",
      "   - Production: Sample or trace all (depending on volume)\n",
      "\n",
      "3. Combine tracing with experiment tracking\n",
      "   - Traces give you the \"how\"\n",
      "   - Experiments give you the \"what\" and \"why\"\n",
      "\n",
      "4. Review traces regularly\n",
      "   - Look for performance bottlenecks\n",
      "   - Identify expensive operations\n",
      "   - Understand failure patterns\n",
      "\n",
      "5. Use multiple frameworks\n",
      "   - MLflow supports 30+ integrations\n",
      "   - Pick the best tool for each task\n",
      "\n",
      "âŒ DON'T:\n",
      "\n",
      "1. Ignore traces until there's a problem\n",
      "   - Proactive monitoring prevents issues\n",
      "\n",
      "2. Log sensitive data in traces\n",
      "   - PII, credentials, confidential info\n",
      "   - Use data masking if needed\n",
      "\n",
      "3. Assume autologging captures everything\n",
      "   - Custom operations need manual instrumentation\n",
      "   - We'll cover this in the next notebook!\n",
      "\n",
      "4. Over-rely on individual traces\n",
      "   - Look at aggregate patterns\n",
      "   - Use traces + metrics together\n",
      "\n",
      "ğŸ’¡ PRO TIPS:\n",
      "\n",
      "- Set up trace sampling in high-volume production\n",
      "- Archive old traces to manage storage\n",
      "- Use MLflow UI to inspect traces and metrics\n",
      "- Use Claude Code Assistant to inspect, debug, and suggest fixes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           Tracing Best Practices                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ… DO:\n",
    "\n",
    "1. Enable autologging early in development\n",
    "   - Easier to debug from the start\n",
    "   - Understand performance characteristics early\n",
    "\n",
    "2. Use tracing in all environments\n",
    "   - Development: Full tracing for debugging\n",
    "   - Staging: Trace all requests\n",
    "   - Production: Sample or trace all (depending on volume)\n",
    "\n",
    "3. Combine tracing with experiment tracking\n",
    "   - Traces give you the \"how\"\n",
    "   - Experiments give you the \"what\" and \"why\"\n",
    "\n",
    "4. Review traces regularly\n",
    "   - Look for performance bottlenecks\n",
    "   - Identify expensive operations\n",
    "   - Understand failure patterns\n",
    "\n",
    "5. Use multiple frameworks\n",
    "   - MLflow supports 30+ integrations\n",
    "   - Pick the best tool for each task\n",
    "\n",
    "âŒ DON'T:\n",
    "\n",
    "1. Ignore traces until there's a problem\n",
    "   - Proactive monitoring prevents issues\n",
    "\n",
    "2. Log sensitive data in traces\n",
    "   - PII, credentials, confidential info\n",
    "   - Use data masking if needed\n",
    "\n",
    "3. Assume autologging captures everything\n",
    "   - Custom operations need manual instrumentation\n",
    "   - We'll cover this in the next notebook!\n",
    "\n",
    "4. Over-rely on individual traces\n",
    "   - Look at aggregate patterns\n",
    "   - Use traces + metrics together\n",
    "\n",
    "ğŸ’¡ PRO TIPS:\n",
    "\n",
    "- Set up trace sampling in high-volume production\n",
    "- Archive old traces to manage storage\n",
    "- Use MLflow UI to inspect traces and metrics\n",
    "- Use Claude Code Assistant to inspect, debug, and suggest fixes\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. What tracing is and why it's critical for GenAI\n",
    "2. The trace data model (traces, spans, hierarchy)\n",
    "3. Automatic tracing with `mlflow.openai.autolog()`\n",
    "4. How to view and analyze traces in the MLflow UI\n",
    "5. Tracing multi-step workflows\n",
    "6. Framework integration (LangChain example)\n",
    "7. Error tracing and debugging\n",
    "8. Best practices for production use\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **One line of code** enables complete observability\n",
    "- **Automatic capture** of inputs, outputs, and metadata\n",
    "- **Framework agnostic** - works with OpenAI, LangChain, LlamaIndex, etc.\n",
    "- **Essential for debugging** complex LLM workflows\n",
    "- **Production-ready** with sampling and filtering\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Notebook 1.4: Manual Tracing and Advanced Observability**\n",
    "\n",
    "Learn how to:\n",
    "- Create custom spans with `@mlflow.trace`\n",
    "- Add custom attributes to spans\n",
    "- Build hierarchical traces for complex workflows\n",
    "- Trace RAG pipelines with retrieval and generation steps\n",
    "- Trace agentic workflows with tool usage\n",
    "- Debug production issues with advanced techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
